# Facial-Emotion-Recognition-Using-CNN
The steps of the method are as follows. In the first phase, the method takes the input image and it checks for face region in the image. If the face is detected, then it applies image processing techniques to extract features and provides it to next step for training the neural network.   Facial features extraction is a process of locating specific regions, point, landmarks or curves in 2D or 3D image. This is actually done with the help of OpenCV library which consists haar cascade classifier and pre-trained facial landmark predictor, haar features helps to identify the features such as lips, eyes, eyebrows, nose etc.   CNN architectures are used for facial expression recognition, the input images considered are of size 48x48 pixels. Architectures are composed of convolution layers, pooling layers, and fully connected layers. After each convolutional layer and fully connected layer (except the output layer), the activation function is applied. The output layer consists of 7 neurons corresponding to 7 emotional labels: angry, disgust, fear, happy, sad, surprise and neutral.      We have used two different datasets for different stages. This model is first fitted to a training set which is made from collecting various examples that are used to match the limitations of the model. The fitted model will normally use to predict the responses of the second dataset i.e., validation dataset observations. The validation dataset offers an  assessment of a model that is fit for the training dataset. For regularization we may use early stopping of validation of datasets. This dataset comprises 35,887 face crops with 28,821 training and 7066 validation photos. Images are resolutions of 48x48 pixels, and grayscale. This dataset 's human accuracy was about 70%.
